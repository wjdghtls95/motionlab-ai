"""
MotionLab AI - LLM í”¼ë“œë°± ìƒì„± (YAML í”„ë¡¬í”„íŠ¸ + ìë™ ë²„ì „ ê´€ë¦¬)
"""

import json
from typing import Dict, Any, List

from openai import AsyncOpenAI

from config import get_settings
from core.prompts.loader import prompt_loader
from utils.logger import logger
from utils.exceptions import (
    LLMGenerationError,
    LLMParseError,
    LLMInvalidResponseError,
)


class LLMFeedback:
    """OpenAI GPT-4o-minië¥¼ ì‚¬ìš©í•œ í”¼ë“œë°± ìƒì„±"""

    def __init__(self):
        """LLMFeedback ì´ˆê¸°í™”"""
        settings = get_settings()
        self.noop_mode = settings.ENABLE_LLM_NOOP

        if not self.noop_mode:
            self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
            self.model = "gpt-4o-mini"
            logger.info(f"âœ… LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: model={self.model}")
        else:
            self.client = None
            self.model = None
            logger.warning("âš ï¸ LLM NOOP ëª¨ë“œ í™œì„±í™” (ê·œì¹™ ê¸°ë°˜ í”¼ë“œë°± ì‚¬ìš©)")

        logger.info(f"âœ… LLMFeedback ì´ˆê¸°í™”: model={self.model}")

    async def generate_feedback(
        self,
        sport_type: str,
        sub_category: str,
        angles: Dict[str, float],
        phases: List[Dict[str, Any]],
        sport_config: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """LLM í”¼ë“œë°± ìƒì„±"""

        if self.noop_mode:
            logger.info("ğŸ”„ NOOP ëª¨ë“œ: ê·œì¹™ ê¸°ë°˜ í”¼ë“œë°± ìƒì„±")
            return self._generate_rule_based_feedback(
                sport_type=sport_type,
                sub_category=sub_category,
                angles=angles,
                phases=phases,
                sport_config=sport_config,
            )

        try:
            messages = self._build_prompt(
                sport_type, sub_category, angles, phases, sport_config
            )

            logger.info(
                f"ğŸ“¤ LLM í˜¸ì¶œ: {sport_type}/{sub_category}, "
                f"prompt_version={messages['version']}"
            )

            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": messages["system"]},
                    {"role": "user", "content": messages["user"]},
                ],
                response_format={"type": "json_object"},
                temperature=0.7,
                max_tokens=1000,
            )

            content = response.choices[0].message.content

            # ========== JSON íŒŒì‹± ì˜ˆì™¸ ì²˜ë¦¬ ê°œì„  ==========
            try:
                result = json.loads(content)
            except json.JSONDecodeError as e:
                logger.error(f"âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
                raise LLMParseError(
                    details=f"Invalid JSON from LLM: {str(e)}, response: {content[:200]}"
                )

            # ========== ì‘ë‹µ êµ¬ì¡° ê²€ì¦ ==========
            required_keys = ["feedback", "overall_score", "improvements"]
            missing_keys = [key for key in required_keys if key not in result]
            if missing_keys:
                logger.error(f"âŒ LLM ì‘ë‹µ ê²€ì¦ ì‹¤íŒ¨: missing keys={missing_keys}")
                raise LLMInvalidResponseError(
                    details=f"Missing required keys: {missing_keys}"
                )

            result["prompt_version"] = messages["version"]

            logger.info(
                f"âœ… LLM ì‘ë‹µ: score={result.get('overall_score')}, "
                f"version={messages['version']}"
            )

            return result

        except (LLMParseError, LLMInvalidResponseError):
            raise

        except Exception as e:
            logger.error(f"âŒ LLM í”¼ë“œë°± ìƒì„± ì‹¤íŒ¨: {e}")
            raise LLMGenerationError(
                details=f"Unexpected error during LLM generation: {str(e)}"
            )

    def _build_prompt(
        self,
        sport_type: str,
        sub_category: str,
        angles: Dict[str, float],
        phases: List[Dict[str, Any]],
        sport_config: Dict[str, Any] = None,
    ) -> Dict[str, str]:
        # anglesì— ideal_rangeë¥¼ í¬í•¨ì‹œì¼œì„œ ë„˜ê¸°ê¸°
        angle_configs = sport_config.get("angles", {}) if sport_config else {}

        angles_list = []
        for name, value in angles.items():
            angle_data = {
                "name": name,
                "value": value,
            }
            # configì—ì„œ ideal_range ê°€ì ¸ì˜¤ê¸°
            if name in angle_configs:
                angle_data["ideal_range"] = angle_configs[name].get("ideal_range")
                angle_data["description"] = angle_configs[name].get("description", "")
            angles_list.append(angle_data)

        return prompt_loader.load(
            sport_type=sport_type,
            sub_category=sub_category,
            context={"angles": angles_list, "phases": phases},
        )

    def _generate_rule_based_feedback(
        self,
        sport_type: str,
        sub_category: str,
        angles: Dict[str, float],
        phases: List[Dict[str, Any]],
        sport_config: Dict[str, Any] = None,
    ) -> Dict[str, Any]:
        """
        ê·œì¹™ ê¸°ë°˜ í”¼ë“œë°± (JSONì˜ angle_validation ì‚¬ìš©)
        """
        if not angles:
            return {
                "overall_score": 50,
                "feedback": "ê°ë„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "improvements": [
                    {
                        "issue": "ê°ë„ ë°ì´í„° ë¶€ì¡±",
                        "suggestion": "ì˜ìƒì—ì„œ ì‹ ì²´ê°€ ëª…í™•íˆ ë³´ì´ë„ë¡ ì´¬ì˜í•´ì£¼ì„¸ìš”",
                    }
                ],
                "prompt_version": "noop",
            }

        if not sport_config or "angle_validation" not in sport_config:
            logger.error(
                f"âŒ sport_config ë˜ëŠ” angle_validation ëˆ„ë½: {sport_type}/{sub_category}"
            )
            raise ValueError(
                f"sport_config.angle_validationì´ í•„ìš”í•©ë‹ˆë‹¤: {sport_type}/{sub_category}"
            )

        validation = sport_config["angle_validation"]
        angle_feedbacks = sport_config.get("angles", {})

        angle_scores = []
        good_points = []
        improvements = []

        # Dict ìˆœíšŒ
        for angle_name, angle_value in angles.items():
            # í•´ë‹¹ ê°ë„ì˜ validation ê°€ì ¸ì˜¤ê¸°
            angle_valid = validation.get(angle_name)
            if not angle_valid:
                continue

            min_normal = angle_valid["min_normal"]
            max_normal = angle_valid["max_normal"]

            # í•´ë‹¹ ê°ë„ì˜ feedback ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
            angle_config = angle_feedbacks.get(angle_name, {})
            feedback_msgs = angle_config.get("feedback", {})

            if min_normal <= angle_value <= max_normal:
                # ideal_range ì•ˆì— ìˆëŠ”ì§€ë„ ì¶”ê°€ í™•ì¸
                ideal = angle_config.get("ideal_range", [min_normal, max_normal])
                if ideal[0] <= angle_value <= ideal[1]:
                    angle_scores.append(90)
                    msg = feedback_msgs.get("good", f"{angle_name} ì–‘í˜¸")
                    good_points.append(f"{angle_name}: {angle_value:.1f}ë„ â€” {msg}")
                else:
                    angle_scores.append(70)
                    msg = feedback_msgs.get("caution", f"{angle_name} ì£¼ì˜")
                    improvements.append(
                        {
                            "issue": f"{angle_name} ì£¼ì˜",
                            "current_value": angle_value,
                            "ideal_range": ideal,
                            "suggestion": msg,
                        }
                    )
            else:
                angle_scores.append(40)
                msg = feedback_msgs.get("correction", f"{angle_name} êµì • í•„ìš”")
                improvements.append(
                    {
                        "issue": f"{angle_name} ë²”ìœ„ ì´íƒˆ",
                        "current_value": angle_value,
                        "valid_range": [min_normal, max_normal],
                        "suggestion": msg,
                    }
                )

        overall_score = sum(angle_scores) // len(angle_scores) if angle_scores else 70

        feedback_parts = []
        if good_points:
            feedback_parts.append(good_points[0])
        if improvements:
            feedback_parts.append(improvements[0]["suggestion"])

        feedback = (
            " | ".join(feedback_parts)
            if feedback_parts
            else f"{sport_type}/{sub_category} ë¶„ì„ ì™„ë£Œ"
        )

        logger.info(
            f"âœ… ê·œì¹™ ê¸°ë°˜ í”¼ë“œë°±: score={overall_score}, "
            f"good={len(good_points)}, issues={len(improvements)}"
        )

        return {
            "overall_score": overall_score,
            "feedback": feedback,
            "improvements": (
                improvements[:3]
                if improvements
                else [
                    {
                        "issue": "ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸",
                        "suggestion": "í˜„ì¬ ìì„¸ë¥¼ ìœ ì§€í•˜ì„¸ìš”",
                    }
                ]
            ),
            "prompt_version": "noop",
        }
